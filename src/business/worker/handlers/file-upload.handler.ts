/**
 * NAS íŒŒì¼ ì—…ë¡œë“œ í•¸ë“¤ëŸ¬
 *
 * íŒŒì¼ í¬ê¸°ì— ë”°ë¼ ì „ëµ ë¶„ê¸°:
 * - ì†Œìš©ëŸ‰ (< 100MB): ìŠ¤íŠ¸ë¦¼ ë°©ì‹ + ì§„í–‰ë¥  ë¡œê¹…
 * - ëŒ€ìš©ëŸ‰ (>= 100MB): ì²­í¬ ë³‘ë ¬ ì—…ë¡œë“œ + ì§„í–‰ë¥  ë¡œê¹…
 */
import { Injectable, Inject, Logger } from '@nestjs/common';
import { v4 as uuidv4 } from 'uuid';
import { createHash } from 'crypto';
import { PassThrough } from 'stream';
import {
  PROGRESS_STORAGE_PORT,
  type IProgressStoragePort,
} from '../../../domain/queue/ports/progress-storage.port';
import {
  CACHE_STORAGE_PORT,
} from '../../../domain/storage/ports/cache-storage.port';
import {
  NAS_STORAGE_PORT,
} from '../../../domain/storage/ports/nas-storage.port';
import { AvailabilityStatus } from '../../../domain/file';
import { FileDomainService } from '../../../domain/file/service/file-domain.service';
import { FileNasStorageDomainService } from '../../../domain/storage/file/service/file-nas-storage-domain.service';
import { FileCacheStorageDomainService } from '../../../domain/storage/file/service/file-cache-storage-domain.service';
import { UploadSessionDomainService } from '../../../domain/upload-session/service/upload-session-domain.service';
import { SyncEventLifecycleHelper } from '../shared/sync-event-lifecycle.helper';
import { createProgressStream, createProgressLogger, formatBytes } from '../../../common/utils';
import type { Job } from '../../../domain/queue/ports/job-queue.port';
import type { ICacheStoragePort } from '../../../domain/storage/ports/cache-storage.port';
import type { INasStoragePort } from '../../../domain/storage/ports/nas-storage.port';
import type { NasFileUploadJobData } from '../nas-file-sync.worker';
import { PARALLEL_UPLOAD_CONFIG } from '../nas-file-sync.worker';

@Injectable()
export class FileUploadHandler {
  private readonly logger = new Logger(FileUploadHandler.name);

  constructor(
    @Inject(CACHE_STORAGE_PORT)
    private readonly cacheStorage: ICacheStoragePort,
    @Inject(NAS_STORAGE_PORT)
    private readonly nasStorage: INasStoragePort,
    @Inject(PROGRESS_STORAGE_PORT)
    private readonly progressStorage: IProgressStoragePort,
    private readonly fileDomainService: FileDomainService,
    private readonly fileNasStorageDomainService: FileNasStorageDomainService,
    private readonly fileCacheStorageDomainService: FileCacheStorageDomainService,
    private readonly uploadSessionDomainService: UploadSessionDomainService,
    private readonly syncEventHelper: SyncEventLifecycleHelper,
  ) {}

  async execute(job: Job<NasFileUploadJobData>): Promise<void> {
    const { fileId, syncEventId, multipartSessionId, compositeChecksum } = job.data;
    this.logger.debug(`íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹œì‘: ${fileId}${multipartSessionId ? ' (multipart)' : ''}`);

    const syncEvent = await this.syncEventHelper.getSyncEvent(syncEventId);

    try {
      await this.syncEventHelper.markProcessing(syncEvent);

      const nasObject = await this.fileNasStorageDomainService.ì¡°íšŒ(fileId);

      if (!nasObject) {
        this.logger.warn(`NAS ìŠ¤í† ë¦¬ì§€ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: ${fileId}`);
        await this.syncEventHelper.markDone(syncEvent);
        return;
      }

      if (nasObject.isAvailable()) {
        this.logger.debug(`ì´ë¯¸ NASì— ë™ê¸°í™”ëœ íŒŒì¼: ${fileId}`);
        await this.syncEventHelper.markDone(syncEvent);
        return;
      }

      const file = await this.fileDomainService.ì¡°íšŒ(fileId);
      if (!file) {
        this.logger.warn(`íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: ${fileId}`);
        await this.syncEventHelper.markDone(syncEvent);
        return;
      }

      const objectKey = syncEvent?.targetPath || fileId;
      const fileSize = file.sizeBytes;
      const shortFileId = fileId.substring(0, 8);
      const totalChunks = Math.ceil(fileSize / PARALLEL_UPLOAD_CONFIG.CHUNK_SIZE);

      // ì§„í–‰ë¥  ì´ˆê¸°í™” (syncEventIdê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ, ì‹¤íŒ¨í•´ë„ ì£¼ ì‘ì—…ì€ ê³„ì†)
      if (syncEventId) {
        try {
          await this.progressStorage.set(syncEventId, {
            fileId,
            syncEventId,
            eventType: 'CREATE',
            status: 'PROCESSING',
            progress: {
              percent: 0,
              completedChunks: 0,
              totalChunks,
              bytesTransferred: 0,
              totalBytes: fileSize,
            },
            startedAt: new Date().toISOString(),
            updatedAt: new Date().toISOString(),
          });
        } catch (progressError) {
          this.logger.warn(
            `ì§„í–‰ë¥  ì´ˆê¸°í™” ì‹¤íŒ¨ (ì—…ë¡œë“œëŠ” ê³„ì† ì§„í–‰): ${(progressError as Error).message}`,
          );
        }
      }

      if (multipartSessionId) {
        // === ë©€í‹°íŒŒíŠ¸ ê²½ë¡œ: íŒŒíŠ¸ì—ì„œ ì§ì ‘ NAS ì—…ë¡œë“œ ===
        this.logger.log(
          `[MULTIPART_UPLOAD] íŒŒíŠ¸ ê¸°ë°˜ NAS ì—…ë¡œë“œ ì‹œì‘ | file=${shortFileId}... | ` +
          `size=${formatBytes(fileSize)} | session=${multipartSessionId.substring(0, 8)}...`,
        );
        await this.uploadToNasFromParts(multipartSessionId, objectKey, fileSize, syncEventId);

        // NAS ìƒíƒœ ì—…ë°ì´íŠ¸
        nasObject.updateStatus(AvailabilityStatus.AVAILABLE);
        nasObject.updateObjectKey(objectKey);
        await this.fileNasStorageDomainService.ì €ì¥(nasObject);

        // íŒŒíŠ¸ â†’ ìºì‹œ ë‹¨ì¼ íŒŒì¼ concat + SHA-256 ë™ì‹œ ê³„ì‚°
        this.logger.log(
          `[MULTIPART_CONCAT] íŒŒíŠ¸ â†’ ìºì‹œ íŒŒì¼ concat ì‹œì‘ | file=${shortFileId}...`,
        );
        const realChecksum = await this.concatPartsToCache(multipartSessionId, fileId);

        // NAS ìŠ¤í† ë¦¬ì§€ ì²´í¬ì„¬ì„ ì‹¤ì œ SHA-256ìœ¼ë¡œ ê°±ì‹ 
        nasObject.updateChecksum(realChecksum);
        await this.fileNasStorageDomainService.ì €ì¥(nasObject);

        // ìºì‹œ ìŠ¤í† ë¦¬ì§€ ê°ì²´ ìƒì„± (ì‹¤ì œ SHA-256 ì‚¬ìš©)
        await this.fileCacheStorageDomainService.ìƒì„±({
          id: uuidv4(),
          fileId,
          checksum: realChecksum,
        });

        this.logger.log(
          `[MULTIPART_CONCAT] ìºì‹œ íŒŒì¼ ìƒì„± ì™„ë£Œ | file=${shortFileId}... | ` +
          `sha256=${realChecksum.substring(0, 12)}...`,
        );

        // ì„¸ì…˜ ì™„ë£Œ + íŒŒíŠ¸ íŒŒì¼ ì‚­ì œ
        await this.finalizeMultipartSession(multipartSessionId, fileId);
      } else {
        // === ê¸°ì¡´ ê²½ë¡œ: ìºì‹œ íŒŒì¼ì—ì„œ NAS ì—…ë¡œë“œ ===
        if (fileSize >= PARALLEL_UPLOAD_CONFIG.THRESHOLD_BYTES) {
          this.logger.log(
            `[PARALLEL_UPLOAD] ë³‘ë ¬ ì—…ë¡œë“œ ì‹œì‘ | file=${shortFileId}... | ` +
            `size=${formatBytes(fileSize)} | chunks=${totalChunks}`,
          );
          await this.parallelUploadToNas(fileId, objectKey, fileSize, syncEventId);
        } else {
          this.logger.log(
            `[STREAM_UPLOAD] ìŠ¤íŠ¸ë¦¼ ì—…ë¡œë“œ ì‹œì‘ | file=${shortFileId}... | ` +
            `size=${formatBytes(fileSize)}`,
          );
          await this.streamUploadToNas(fileId, objectKey, fileSize);
        }

        nasObject.updateStatus(AvailabilityStatus.AVAILABLE);
        nasObject.updateObjectKey(objectKey);
        await this.fileNasStorageDomainService.ì €ì¥(nasObject);
      }

      // ì§„í–‰ë¥  ì™„ë£Œ ì—…ë°ì´íŠ¸
      if (syncEventId) {
        await this.progressStorage.update(syncEventId, {
          status: 'DONE',
          progress: {
            percent: 100,
            completedChunks: totalChunks,
            totalChunks,
            bytesTransferred: fileSize,
            totalBytes: fileSize,
          },
        });
      }

      await this.syncEventHelper.markDone(syncEvent);
      this.logger.log(
        `[SYNC_COMPLETE] NAS ë™ê¸°í™” ì™„ë£Œ | file=${shortFileId}... | ` +
        `size=${formatBytes(fileSize)} | path=${objectKey}${multipartSessionId ? ' | multipart' : ''}`,
      );
    } catch (error) {
      this.logger.error(`NAS íŒŒì¼ ë™ê¸°í™” ì‹¤íŒ¨: ${fileId}`, error);

      if (syncEventId) {
        try {
          await this.progressStorage.update(syncEventId, {
            status: 'FAILED',
            error: (error as Error).message,
          });
        } catch (progressError) {
          this.logger.warn(
            `ì§„í–‰ë¥  ì‹¤íŒ¨ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: ${(progressError as Error).message}`,
          );
        }
      }

      await this.syncEventHelper.handleRetry(
        syncEvent,
        error as Error,
        `action=upload | fileId=${fileId}`,
      );
      throw error;
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¼ ë°©ì‹ ì—…ë¡œë“œ (ì†Œìš©ëŸ‰ íŒŒì¼ìš©)
   */
  private async streamUploadToNas(
    fileId: string,
    objectKey: string,
    fileSize: number,
  ): Promise<void> {
    const readStream = await this.cacheStorage.íŒŒì¼ìŠ¤íŠ¸ë¦¼ì½ê¸°(fileId);

    const { callback: progressCallback } = createProgressLogger(
      this.logger,
      fileId,
      'NAS_SYNC',
      PARALLEL_UPLOAD_CONFIG.PROGRESS_LOG_INTERVAL,
    );

    const progressStream = createProgressStream(fileSize, progressCallback);
    await this.nasStorage.íŒŒì¼ìŠ¤íŠ¸ë¦¼ì“°ê¸°(objectKey, readStream.pipe(progressStream));
  }

  /**
   * ì²­í¬ ë³‘ë ¬ ì—…ë¡œë“œ (ëŒ€ìš©ëŸ‰ íŒŒì¼ìš©)
   */
  private async parallelUploadToNas(
    fileId: string,
    objectKey: string,
    fileSize: number,
    syncEventId?: string,
  ): Promise<void> {
    const { CHUNK_SIZE, PARALLEL_CHUNKS, PROGRESS_LOG_INTERVAL } = PARALLEL_UPLOAD_CONFIG;
    const shortFileId = fileId.substring(0, 8);

    await this.nasStorage.íŒŒì¼ì‚¬ì „í• ë‹¹(objectKey, fileSize);

    const totalChunks = Math.ceil(fileSize / CHUNK_SIZE);
    const chunks: Array<{ index: number; start: number; end: number }> = [];

    for (let i = 0; i < totalChunks; i++) {
      const start = i * CHUNK_SIZE;
      const end = Math.min(start + CHUNK_SIZE - 1, fileSize - 1);
      chunks.push({ index: i, start, end });
    }

    let completedChunks = 0;
    let lastLoggedPercent = 0;

    const processChunk = async (chunk: { index: number; start: number; end: number }) => {
      const { start, end } = chunk;

      const chunkStream = await this.cacheStorage.íŒŒì¼ë²”ìœ„ìŠ¤íŠ¸ë¦¼ì½ê¸°(fileId, start, end);

      const buffers: Buffer[] = [];
      for await (const data of chunkStream) {
        buffers.push(data);
      }
      const chunkBuffer = Buffer.concat(buffers);

      await this.nasStorage.ì²­í¬ì“°ê¸°(objectKey, chunkBuffer, start);

      completedChunks++;
      const percent = Math.round((completedChunks / totalChunks) * 100);
      const bytesTransferred = Math.min(completedChunks * CHUNK_SIZE, fileSize);

      if (percent >= lastLoggedPercent + PROGRESS_LOG_INTERVAL || percent === 100) {
        if (syncEventId) {
          await this.progressStorage.update(syncEventId, {
            status: 'PROCESSING',
            progress: {
              percent,
              completedChunks,
              totalChunks,
              bytesTransferred,
              totalBytes: fileSize,
            },
          });
        }

        this.logger.log(
          `[PARALLEL_UPLOAD] ğŸ“Š ì§„í–‰ë¥  | file=${shortFileId}... | ${percent}% | ` +
          `chunks=${completedChunks}/${totalChunks}`,
        );
        lastLoggedPercent = Math.floor(percent / PROGRESS_LOG_INTERVAL) * PROGRESS_LOG_INTERVAL;
      }
    };

    const executeInBatches = async () => {
      for (let i = 0; i < chunks.length; i += PARALLEL_CHUNKS) {
        const batch = chunks.slice(i, i + PARALLEL_CHUNKS);
        await Promise.all(batch.map(processChunk));
      }
    };

    await executeInBatches();

    this.logger.log(
      `[PARALLEL_UPLOAD] ì „ì²´ ì²­í¬ ì—…ë¡œë“œ ì™„ë£Œ | file=${shortFileId}... | ` +
      `totalChunks=${totalChunks}`,
    );
  }

  /**
   * ë©€í‹°íŒŒíŠ¸: íŒŒíŠ¸ íŒŒì¼ì—ì„œ ì§ì ‘ NASë¡œ ë³‘ë ¬ ì—…ë¡œë“œ
   *
   * íŒŒíŠ¸(10MB) â†’ NAS ì²­í¬(50MB) ë§¤í•‘:
   * - NAS ì²­í¬ 1ê°œ = íŒŒíŠ¸ 5ê°œ (50MB / 10MB)
   * - íŒŒíŠ¸ë¥¼ ìˆœì°¨ ì½ì–´ Bufferë¡œ ì¡°í•© â†’ NAS ì²­í¬ì“°ê¸°
   */
  private async uploadToNasFromParts(
    sessionId: string,
    objectKey: string,
    fileSize: number,
    syncEventId?: string,
  ): Promise<void> {
    const parts = await this.uploadSessionDomainService.ì™„ë£ŒíŒŒíŠ¸ëª©ë¡ì¡°íšŒ(sessionId);
    const sortedParts = parts.sort((a, b) => a.partNumber - b.partNumber);
    const { CHUNK_SIZE, PARALLEL_CHUNKS, PROGRESS_LOG_INTERVAL } = PARALLEL_UPLOAD_CONFIG;
    const shortSessionId = sessionId.substring(0, 8);

    await this.nasStorage.íŒŒì¼ì‚¬ì „í• ë‹¹(objectKey, fileSize);

    // íŒŒíŠ¸ í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ NAS ì²­í¬ ë‹¹ íŒŒíŠ¸ ìˆ˜ ê³„ì‚°
    const partSize = sortedParts[0]?.size || (10 * 1024 * 1024);
    const partsPerChunk = Math.ceil(CHUNK_SIZE / partSize);
    const totalChunks = Math.ceil(fileSize / CHUNK_SIZE);

    let completedChunks = 0;
    let lastLoggedPercent = 0;

    const processChunk = async (chunkIndex: number) => {
      const startIdx = chunkIndex * partsPerChunk;
      const chunkParts = sortedParts.slice(startIdx, startIdx + partsPerChunk);

      const buffers: Buffer[] = [];
      for (const part of chunkParts) {
        if (!part.objectKey) continue;
        const stream = await this.cacheStorage.íŒŒì¼ìŠ¤íŠ¸ë¦¼ì½ê¸°(part.objectKey);
        for await (const data of stream) {
          buffers.push(data);
        }
      }

      const offset = chunkIndex * CHUNK_SIZE;
      await this.nasStorage.ì²­í¬ì“°ê¸°(objectKey, Buffer.concat(buffers), offset);

      completedChunks++;
      const percent = Math.round((completedChunks / totalChunks) * 100);
      const bytesTransferred = Math.min(completedChunks * CHUNK_SIZE, fileSize);

      if (percent >= lastLoggedPercent + PROGRESS_LOG_INTERVAL || percent === 100) {
        if (syncEventId) {
          await this.progressStorage.update(syncEventId, {
            status: 'PROCESSING',
            progress: {
              percent,
              completedChunks,
              totalChunks,
              bytesTransferred,
              totalBytes: fileSize,
            },
          });
        }

        this.logger.log(
          `[MULTIPART_UPLOAD] ì§„í–‰ë¥  | session=${shortSessionId}... | ${percent}% | ` +
          `chunks=${completedChunks}/${totalChunks}`,
        );
        lastLoggedPercent = Math.floor(percent / PROGRESS_LOG_INTERVAL) * PROGRESS_LOG_INTERVAL;
      }
    };

    // ë°°ì¹˜ ì‹¤í–‰ (ê¸°ì¡´ íŒ¨í„´ ë™ì¼)
    for (let i = 0; i < totalChunks; i += PARALLEL_CHUNKS) {
      const batch = Array.from(
        { length: Math.min(PARALLEL_CHUNKS, totalChunks - i) },
        (_, j) => i + j,
      );
      await Promise.all(batch.map(processChunk));
    }

    this.logger.log(
      `[MULTIPART_UPLOAD] NAS ì—…ë¡œë“œ ì™„ë£Œ | session=${shortSessionId}... | ` +
      `totalChunks=${totalChunks}`,
    );
  }

  /**
   * ë©€í‹°íŒŒíŠ¸: íŒŒíŠ¸ë¥¼ ë¡œì»¬ì—ì„œ concatí•˜ì—¬ ìºì‹œ ë‹¨ì¼ íŒŒì¼ ìƒì„±
   *
   * ì¶”ê°€ I/O ì—†ì´ concat ì¤‘ SHA-256ì„ ë™ì‹œ ê³„ì‚°í•˜ì—¬ ë°˜í™˜.
   * SSD ê¸°ì¤€ 1GBë‹¹ ~1-2ì´ˆ ì†Œìš”.
   *
   * @returns ì‹¤ì œ íŒŒì¼ ë°”ì´ë„ˆë¦¬ì˜ SHA-256 í•´ì‹œ
   */
  private async concatPartsToCache(sessionId: string, fileId: string): Promise<string> {
    const parts = await this.uploadSessionDomainService.ì™„ë£ŒíŒŒíŠ¸ëª©ë¡ì¡°íšŒ(sessionId);
    const sorted = parts.sort((a, b) => a.partNumber - b.partNumber);

    const hash = createHash('sha256');
    const mergeStream = new PassThrough();
    const writePromise = this.cacheStorage.íŒŒì¼ìŠ¤íŠ¸ë¦¼ì“°ê¸°(fileId, mergeStream);

    try {
      for (const part of sorted) {
        if (!part.objectKey) continue;

        const partStream = await this.cacheStorage.íŒŒì¼ìŠ¤íŠ¸ë¦¼ì½ê¸°(part.objectKey);

        // backpressure ì²˜ë¦¬í•˜ë©° íŒŒíŠ¸ â†’ mergeStream ì „ë‹¬ + SHA-256 ë™ì‹œ ê³„ì‚°
        await new Promise<void>((resolve, reject) => {
          partStream.on('data', (chunk: Buffer) => {
            hash.update(chunk);
            if (!mergeStream.write(chunk)) {
              partStream.pause();
              mergeStream.once('drain', () => partStream.resume());
            }
          });
          partStream.on('end', resolve);
          partStream.on('error', reject);
        });
      }

      mergeStream.end();
      await writePromise;
    } catch (error) {
      if (!mergeStream.destroyed) {
        mergeStream.destroy();
      }
      throw error;
    }

    return hash.digest('hex');
  }

  /**
   * ë©€í‹°íŒŒíŠ¸: ì„¸ì…˜ ì™„ë£Œ ì²˜ë¦¬ + íŒŒíŠ¸ íŒŒì¼ ì‚­ì œ
   */
  private async finalizeMultipartSession(sessionId: string, fileId: string): Promise<void> {
    // 1. ì„¸ì…˜ COMPLETING â†’ COMPLETED
    const session = await this.uploadSessionDomainService.ì„¸ì…˜ì¡°íšŒ(sessionId);
    if (session?.isCompleting()) {
      session.complete(fileId);
      await this.uploadSessionDomainService.ì„¸ì…˜ì €ì¥(session);
      this.logger.debug(`ì„¸ì…˜ ì™„ë£Œ ì²˜ë¦¬: ${sessionId}`);
    }

    // 2. íŒŒíŠ¸ íŒŒì¼ ì‚­ì œ
    const parts = await this.uploadSessionDomainService.ì™„ë£ŒíŒŒíŠ¸ëª©ë¡ì¡°íšŒ(sessionId);
    for (const part of parts) {
      if (part.objectKey) {
        try {
          await this.cacheStorage.íŒŒì¼ì‚­ì œ(part.objectKey);
        } catch (e) {
          this.logger.warn(`íŒŒíŠ¸ ì‚­ì œ ì‹¤íŒ¨: ${part.objectKey} - ${(e as Error).message}`);
        }
      }
    }

    // 3. ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì‚­ì œ (cache/multipart/{sessionId}/)
    try {
      await this.cacheStorage.ë””ë ‰í† ë¦¬ì‚­ì œ(`multipart/${sessionId}`);
    } catch (e) {
      this.logger.warn(`ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨: ${(e as Error).message}`);
    }

    this.logger.debug(`íŒŒíŠ¸ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: ${sessionId} (${parts.length}ê°œ)`);
  }
}
