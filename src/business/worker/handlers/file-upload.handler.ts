/**
 * NAS íŒŒì¼ ì—…ë¡œë“œ í•¸ë“¤ëŸ¬
 *
 * íŒŒì¼ í¬ê¸°ì— ë”°ë¼ ì „ëµ ë¶„ê¸°:
 * - ì†Œìš©ëŸ‰ (< 100MB): ìŠ¤íŠ¸ë¦¼ ë°©ì‹ + ì§„í–‰ë¥  ë¡œê¹…
 * - ëŒ€ìš©ëŸ‰ (>= 100MB): ì²­í¬ ë³‘ë ¬ ì—…ë¡œë“œ + ì§„í–‰ë¥  ë¡œê¹…
 */
import { Injectable, Inject, Logger } from '@nestjs/common';
import {
  PROGRESS_STORAGE_PORT,
  type IProgressStoragePort,
} from '../../../domain/queue/ports/progress-storage.port';
import {
  CACHE_STORAGE_PORT,
} from '../../../domain/storage/ports/cache-storage.port';
import {
  NAS_STORAGE_PORT,
} from '../../../domain/storage/ports/nas-storage.port';
import { AvailabilityStatus } from '../../../domain/file';
import { FileDomainService } from '../../../domain/file/service/file-domain.service';
import { FileNasStorageDomainService } from '../../../domain/storage/file/service/file-nas-storage-domain.service';
import { SyncEventLifecycleHelper } from '../shared/sync-event-lifecycle.helper';
import { createProgressStream, createProgressLogger, formatBytes } from '../../../common/utils';
import type { Job } from '../../../domain/queue/ports/job-queue.port';
import type { ICacheStoragePort } from '../../../domain/storage/ports/cache-storage.port';
import type { INasStoragePort } from '../../../domain/storage/ports/nas-storage.port';
import type { NasFileUploadJobData } from '../nas-file-sync.worker';
import { PARALLEL_UPLOAD_CONFIG } from '../nas-file-sync.worker';

@Injectable()
export class FileUploadHandler {
  private readonly logger = new Logger(FileUploadHandler.name);

  constructor(
    @Inject(CACHE_STORAGE_PORT)
    private readonly cacheStorage: ICacheStoragePort,
    @Inject(NAS_STORAGE_PORT)
    private readonly nasStorage: INasStoragePort,
    @Inject(PROGRESS_STORAGE_PORT)
    private readonly progressStorage: IProgressStoragePort,
    private readonly fileDomainService: FileDomainService,
    private readonly fileNasStorageDomainService: FileNasStorageDomainService,
    private readonly syncEventHelper: SyncEventLifecycleHelper,
  ) {}

  async execute(job: Job<NasFileUploadJobData>): Promise<void> {
    const { fileId, syncEventId } = job.data;
    this.logger.debug(`íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹œì‘: ${fileId}`);

    const syncEvent = await this.syncEventHelper.getSyncEvent(syncEventId);

    try {
      await this.syncEventHelper.markProcessing(syncEvent);

      const nasObject = await this.fileNasStorageDomainService.ì¡°íšŒ(fileId);

      if (!nasObject) {
        this.logger.warn(`NAS ìŠ¤í† ë¦¬ì§€ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: ${fileId}`);
        await this.syncEventHelper.markDone(syncEvent);
        return;
      }

      if (nasObject.isAvailable()) {
        this.logger.debug(`ì´ë¯¸ NASì— ë™ê¸°í™”ëœ íŒŒì¼: ${fileId}`);
        await this.syncEventHelper.markDone(syncEvent);
        return;
      }

      const file = await this.fileDomainService.ì¡°íšŒ(fileId);
      if (!file) {
        this.logger.warn(`íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: ${fileId}`);
        await this.syncEventHelper.markDone(syncEvent);
        return;
      }

      const objectKey = syncEvent?.targetPath || fileId;
      const fileSize = file.sizeBytes;
      const shortFileId = fileId.substring(0, 8);
      const totalChunks = Math.ceil(fileSize / PARALLEL_UPLOAD_CONFIG.CHUNK_SIZE);

      // ì§„í–‰ë¥  ì´ˆê¸°í™” (syncEventIdê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
      if (syncEventId) {
        await this.progressStorage.set(syncEventId, {
          fileId,
          syncEventId,
          eventType: 'CREATE',
          status: 'PROCESSING',
          progress: {
            percent: 0,
            completedChunks: 0,
            totalChunks,
            bytesTransferred: 0,
            totalBytes: fileSize,
          },
          startedAt: new Date().toISOString(),
          updatedAt: new Date().toISOString(),
        });
      }

      // íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ì „ëµ ë¶„ê¸°
      if (fileSize >= PARALLEL_UPLOAD_CONFIG.THRESHOLD_BYTES) {
        this.logger.log(
          `[PARALLEL_UPLOAD] ğŸš€ ë³‘ë ¬ ì—…ë¡œë“œ ì‹œì‘ | file=${shortFileId}... | ` +
          `size=${formatBytes(fileSize)} | chunks=${Math.ceil(fileSize / PARALLEL_UPLOAD_CONFIG.CHUNK_SIZE)}`,
        );
        await this.parallelUploadToNas(fileId, objectKey, fileSize, syncEventId);
      } else {
        this.logger.log(
          `[STREAM_UPLOAD] ğŸš€ ìŠ¤íŠ¸ë¦¼ ì—…ë¡œë“œ ì‹œì‘ | file=${shortFileId}... | ` +
          `size=${formatBytes(fileSize)}`,
        );
        await this.streamUploadToNas(fileId, objectKey, fileSize);
      }

      nasObject.updateStatus(AvailabilityStatus.AVAILABLE);
      nasObject.updateObjectKey(objectKey);
      await this.fileNasStorageDomainService.ì €ì¥(nasObject);

      // ì§„í–‰ë¥  ì™„ë£Œ ì—…ë°ì´íŠ¸
      if (syncEventId) {
        await this.progressStorage.update(syncEventId, {
          status: 'DONE',
          progress: {
            percent: 100,
            completedChunks: totalChunks,
            totalChunks,
            bytesTransferred: fileSize,
            totalBytes: fileSize,
          },
        });
      }

      await this.syncEventHelper.markDone(syncEvent);
      this.logger.log(
        `[SYNC_COMPLETE] âœ… NAS ë™ê¸°í™” ì™„ë£Œ | file=${shortFileId}... | ` +
        `size=${formatBytes(fileSize)} | path=${objectKey}`,
      );
    } catch (error) {
      this.logger.error(`NAS íŒŒì¼ ë™ê¸°í™” ì‹¤íŒ¨: ${fileId}`, error);

      if (syncEventId) {
        await this.progressStorage.update(syncEventId, {
          status: 'FAILED',
          error: (error as Error).message,
        });
      }

      await this.syncEventHelper.handleRetry(
        syncEvent,
        error as Error,
        `action=upload | fileId=${fileId}`,
      );
      throw error;
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¼ ë°©ì‹ ì—…ë¡œë“œ (ì†Œìš©ëŸ‰ íŒŒì¼ìš©)
   */
  private async streamUploadToNas(
    fileId: string,
    objectKey: string,
    fileSize: number,
  ): Promise<void> {
    const readStream = await this.cacheStorage.íŒŒì¼ìŠ¤íŠ¸ë¦¼ì½ê¸°(fileId);

    const { callback: progressCallback } = createProgressLogger(
      this.logger,
      fileId,
      'NAS_SYNC',
      PARALLEL_UPLOAD_CONFIG.PROGRESS_LOG_INTERVAL,
    );

    const progressStream = createProgressStream(fileSize, progressCallback);
    await this.nasStorage.íŒŒì¼ìŠ¤íŠ¸ë¦¼ì“°ê¸°(objectKey, readStream.pipe(progressStream));
  }

  /**
   * ì²­í¬ ë³‘ë ¬ ì—…ë¡œë“œ (ëŒ€ìš©ëŸ‰ íŒŒì¼ìš©)
   */
  private async parallelUploadToNas(
    fileId: string,
    objectKey: string,
    fileSize: number,
    syncEventId?: string,
  ): Promise<void> {
    const { CHUNK_SIZE, PARALLEL_CHUNKS, PROGRESS_LOG_INTERVAL } = PARALLEL_UPLOAD_CONFIG;
    const shortFileId = fileId.substring(0, 8);

    await this.nasStorage.íŒŒì¼ì‚¬ì „í• ë‹¹(objectKey, fileSize);

    const totalChunks = Math.ceil(fileSize / CHUNK_SIZE);
    const chunks: Array<{ index: number; start: number; end: number }> = [];

    for (let i = 0; i < totalChunks; i++) {
      const start = i * CHUNK_SIZE;
      const end = Math.min(start + CHUNK_SIZE - 1, fileSize - 1);
      chunks.push({ index: i, start, end });
    }

    let completedChunks = 0;
    let lastLoggedPercent = 0;

    const processChunk = async (chunk: { index: number; start: number; end: number }) => {
      const { start, end } = chunk;

      const chunkStream = await this.cacheStorage.íŒŒì¼ë²”ìœ„ìŠ¤íŠ¸ë¦¼ì½ê¸°(fileId, start, end);

      const buffers: Buffer[] = [];
      for await (const data of chunkStream) {
        buffers.push(data);
      }
      const chunkBuffer = Buffer.concat(buffers);

      await this.nasStorage.ì²­í¬ì“°ê¸°(objectKey, chunkBuffer, start);

      completedChunks++;
      const percent = Math.round((completedChunks / totalChunks) * 100);
      const bytesTransferred = Math.min(completedChunks * CHUNK_SIZE, fileSize);

      if (percent >= lastLoggedPercent + PROGRESS_LOG_INTERVAL || percent === 100) {
        if (syncEventId) {
          await this.progressStorage.update(syncEventId, {
            status: 'PROCESSING',
            progress: {
              percent,
              completedChunks,
              totalChunks,
              bytesTransferred,
              totalBytes: fileSize,
            },
          });
        }

        this.logger.log(
          `[PARALLEL_UPLOAD] ğŸ“Š ì§„í–‰ë¥  | file=${shortFileId}... | ${percent}% | ` +
          `chunks=${completedChunks}/${totalChunks}`,
        );
        lastLoggedPercent = Math.floor(percent / PROGRESS_LOG_INTERVAL) * PROGRESS_LOG_INTERVAL;
      }
    };

    const executeInBatches = async () => {
      for (let i = 0; i < chunks.length; i += PARALLEL_CHUNKS) {
        const batch = chunks.slice(i, i + PARALLEL_CHUNKS);
        await Promise.all(batch.map(processChunk));
      }
    };

    await executeInBatches();

    this.logger.log(
      `[PARALLEL_UPLOAD] âœ… ì „ì²´ ì²­í¬ ì—…ë¡œë“œ ì™„ë£Œ | file=${shortFileId}... | ` +
      `totalChunks=${totalChunks}`,
    );
  }
}
